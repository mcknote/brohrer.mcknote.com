# 五種可以用機器學習回答的問題

**Five Questions Data Science Answers**

Translated from Brandon Rohrer's Blog by Jimmy Lin

![](https://brohrer.github.io/images/magnifying_glass.png )

雖然機器學習聽起來很讚，但現階段它其實只能用來解決五種問題：

1. **這是甲，還是乙？**
2. **這有什麼奇怪的嗎？**
3. **這有多少／這有幾個？**
4. **（資料）的組成為何？**
5. **我接下來應該做什麼？**

機器學習是驅使資料科學進步的主要動力。每種方法（也稱作**演算法**，algorithm）都會用作接收和處理資料，並給出一個答案。這些演算法負責了資料科學中最難解釋也最有趣的部分，也就是數學的奧秘。

取決於演算法所能回答的問題，它們可以被分成幾類。這些分類可以幫你理清思路和問對的問題。

## 這是甲，還是乙？

這一類演算法都常被稱作**二元分類**（two-class classification），被用來解決只有兩種結果的問題：是或否、開或關、抽煙或不抽煙、買或不買等等。有很多資料科學上的問題都屬於二元分類，或是可以被轉換為二元分類。這是最簡單也最常見的資料科學問題，以下是一些常見的例子：

* 這位顧客會不會續約？
* 這是一張貓還是狗的圖片？
* 這位顧客會不會點最上面的連結？
* 如果繼續開一千英里，這個輪胎會不會爆胎？
* 抵五元或打七五折，哪一個促銷手段能吸引更多顧客？

## 這是甲、乙、丙還是丁？

這種演算法被稱為**多元分類**（multi-class classification），顧名思義，它可以用來解決有多種（或很多種）回答的問題，例如：哪種口味、哪個人、哪個部分、哪間公司、哪位參選人。大部分的多元分類演算法只是二元分類的延伸。以下是一些常見的例子：

* 這是哪種動物的圖片？
* 這種雷達訊號是來自哪種飛機？
* 這篇新聞的標題該怎麼下？
* 這則推特（twitter）所包含的情緒為何？
* 這則錄音裡的講者是誰？

## 這有什麼奇怪的嗎？

這一類的演算法通常被用來**偵測異狀**（anomaly detection），也就是用來辨別不正常的資料。如果讀者仔細想想，可能會發現這和二元分類的問題差不多，因為它也是用來回答有或沒有（異狀）。不過兩者的區別在於二元分類中的資料中包含兩種回答，但異狀偵測則不一定（，可能只有其中一種）。當讀者想分析的情況發生率很低，導致樣本數也很少的時候，異狀偵測就顯得特別有用，例如用來分析設備故障；或在異常情況有很多種的時候，這種演算法也很有用，例如偵測信用卡盜刷。這裡有一些常見的異狀偵測問題：

* 這個壓力大小有任何異狀嗎？
* 這則網路訊息正常嗎？
* 這些消費跟這位使用者過去的行為落差很大嗎？
* 這些用電量在這個季節和時間算是正常的嗎？

## 這有多少／這有幾個？

當讀者解決的問題涉及數字而非分類時，這一類的演算法就稱為**迴歸**（regression），例如：

* 下週二的氣溫為何？
* 我在葡萄牙的第四季度銷量會有多少？
* 三十分鐘後，我的風力發電廠會有多少千瓦（kW）的需求？
* 我下週會獲得多少新追蹤者？
* 每一千個使用這種軸承的產品裡，有多少個能撐過一萬小時的使用？

一般來說，迴歸演算法會給出一個實數解；這些解可以小到小數點後數位，也可以是負的。對於那些特別問「有幾個」的問題，負數解可能被直接當零，而分數解會被換成最接近的整數。

### 用迴歸演算法解決多元分類問題

有些看起來很像多元分類的問題，其實更適合用迴歸解決。例如，「讀者對哪則新聞最感興趣」乍看之下是個分類問題——從一些新聞中選出一則；但如果將問題換成「對讀者來說，每則新聞的有趣程度為何」並為每則新聞評分，接下來就只需要選出最高分的新聞。這類問題通常和排名或對比有關。

同理，「我的車隊中，哪台廂型車最需要保養」可以換成「我的車隊裡，每台廂型車需要保養的程度為何」；「哪 5% 的顧客隔年會跳槽到對手公司」可以換成「每名顧客隔年跳槽到對手公司的機率為何」。

### 用迴歸演算法解決二元分類問題

可想而知，二元分類問題也可以換成迴歸問題。（事實上，如果探究原理，某些演算法的確會先將二元分類問題直接轉成迴歸來解。）這種作法在二分不盡完美、或兩者都有可能的情況下特別有用。當回答為「一部分是、一部分不是」或「有可能開、也有可能關」的時候，迴歸演算法可以反映這個特性。這類問題也通常以「有多少可能性」、「有多少比例」開頭。

* 這位使用者有多大機率會點我的廣告？
* 這台拉霸機有多少比例的回合會給獎金？
* 這名員工有多大機率會成為內部安全風險？
* 今天有多少比例的航班會準時抵達？

有些讀者可能會注意到二元分類、多元分類、異狀偵測和迴歸等四種演算法之間關係匪淺。它們確實都是**監督式學習**（supervised learning）下的演算法，也有許多相似之處，所以有些問題不只有一種問法，也能用上述多種演算法解決。它們的共通之處，在於建模時都用了一組包含回答的資料（這個過程稱作**訓練**，training），並被用來分類或預測一組不包含回答的資料（這個過程稱作**評分**，scoring）。

除此之外，還有一些不同的資料科學問題，屬於**非監督和強化式學習**（unsupervised and reinforcement learning）類別的處理範疇。

## （資料）的組成為何？

與資料組成相關的問題屬於非監督學習。判斷資料結構的方法有很多，其中一類是**聚類法**（clustering），包括資料群集（chunking）、分組（grouping）、聚束（bunching）、分段（segmentation）等等。這些方法的目的是將資料分成幾個直觀的群體。不同於監督式學習，聚類法所分析的資料不包含任何用來引導分群、說明分群意義和數量的數字或名字。如果說監督式學習是用來在星空中找出幾個特定的星球，聚類法則是用來圈出星空中的星座。由於聚類法可以用來將資料分成「幾叢」群體，分析人員可以更輕易地解讀和解釋資料。

聚類法的基礎是衡量資料之間的距離或相似度，也就是**距離度量**（distance metric）。距離度量可以是任何可測量的數據，例如智商之間的差距、相同基因組的數量、或兩點之間的最短距離。和聚類法相關的問題全都試著將資料分成均等的群體。

* 哪些消費者對農產品有相似的品味？
* 哪些觀眾喜歡同一類電影？
* 哪些型號的印表機有相似的故障？
* 這間變電所在每週的哪幾天有相似的電力需求？
* 用什麼方法可以自然地將這些文件分成五類？

非監督式學習下還有另一類演算法稱作**降維**（dimensionality reduction）。降維是另一種簡化資料的方法，它可以讓資料的溝通變得更容易、處理變得更快、而且存取變得更簡單。

降維的運作原理是創造出一套簡化資料的方法。GPA 就是一個很簡單的例子。雖然每個大學生的學術能力評估都是由四年內無數的課堂和考試所組成，但如果它們全部列出來，沒有任何一位招聘官可以吸收如此龐大的資訊。幸好讀者可以透過計算平均，將這些課堂和考試簡化成 GPA。因為在某門課表現優異的學生，通常其他課程的表現也不錯，這套方法還算管用。只使用 GPA 而非完整成績，確實會喪失一些資訊，像是看不出一名學生的數學程度是否比英文更好，或是比起課堂考試，是否更擅長做回家的程式設計作業；不過這麼做最大的好處是簡約，讓表達和比較學術能力變得非常簡單。

